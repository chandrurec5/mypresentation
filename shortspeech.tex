\documentclass{article}
\newcommand{\ra}{\rightarrow}
\newcommand{\la}{\leftarrow}
\newcommand{\ua}{\uparrow}
\newcommand{\da}{\downarrow}
\renewcommand{\S}{\mathcal{S}}
\newcommand{\p}{\pause}
\newcommand{\X}{\mathcal{X}}
\newcommand{\Y}{\mathcal{Y}}
\newcommand{\A}{\mathcal{A}}
\newcommand{\E}{\mathbf{E}}
\newcommand{\U}{\mathcal{U}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\maxp}{(\max,+)}
\newcommand{\eqdef}{\stackrel{\cdot}{=}}
\newcommand{\tu}{\tilde{u}}
\newcommand{\tj}{\tilde{J}}
\newcommand{\tr}{\tilde{r}}
\newcommand{\minp}{(\min,+)}
\newcommand{\op}{\oplus}
\newcommand{\F}{\mathcal{F}}
\newcommand{\om}{\otimes}
\newcommand{\one}{\mathbf{1}}
\newcommand{\V}{\mathcal{V}}
\newcommand{\mb}{\mbox{ }}
\newcommand{\norm}[1]{\|#1\|}
\newcommand{\inorm}[1]{\|#1\|_{\infty}}
\newcommand{\snorm}[1]{\left\|#1\right\|}
\newcommand{\sinorm}[1]{\left\|#1\right\|_{\infty}}
\newcommand{\N}{\mathcal{N}}
\newcommand{\I}{\mathcal{I}}
\newcommand{\hj}{\hat{J}}
\newcommand{\mn}{$\infty,\psi$}
\newcommand{\hg}{\hat{\Gamma}}
\newcommand{\hr}{\hat{r}}
\newcommand{\tv}{{V}}
\newcommand{\hv}{\hat{V}}
\newcommand{\et}{||\Gamma J^*-\hg J^*||_\infty}
\newcommand{\etmn}{||\Gamma J^*-\hg J^*||_{\mn}}
\usepackage{comment}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{pifont}
\usepackage{nccmath} % mfrac
\usepackage{amsmath,amsthm}
\usepackage{array}
\usepackage{latexsym}
\usepackage{mathtools}
\usepackage{amssymb,amsthm}
\usepackage{amsfonts}



% use Times
\usepackage{times}
% For figures
\usepackage{graphicx} % more modern
%\usepackage{epsfig} % less modern


% For citations
\usepackage{natbib}
\usepackage{amsmath}

% For algorithms
\usepackage{algorithm}
\usepackage{algorithmic}

% As of 2011, we use the hyperref package to produce hyperlinks in the
% resulting PDF.  If this breaks your system, please commend out the
% following usepackage line and replace \usepackage{icml2016} with
% \usepackage[nohyperref]{icml2016} above.
\usepackage{hyperref}

% Packages hyperref and algorithmic misbehave sometimes.  We can fix
% this with the following command.
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Employ the following version of the ``usepackage'' statement for
% submitting the draft version of the paper for review.  This will set
% the note in the first column to ``Under review.  Do not distribute.''
\usepackage{icml2016}

% Employ this version of the ``usepackage'' statement after the paper has
% been accepted, when creating the final version.  This will set the
% note in the first column to ``Proceedings of the...''
%\usepackage[accepted]{icml2016}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{IIT Presentation}

\begin{document}

\onecolumn
\icmltitle{IIT Presentation}
\section{Abstract}
Quite often in life we are presented choices from time to time and the decisions we make not only affects the immediate situation but also influence our future. The framework of Markov Decision Process is useful for us to convert such problems into optimization problems and there are exact algorithms that spell out the right decision rule. However, real world systems are typically large scale and we hit a compuational roadblock and we need to come up with approximate methods. Then natural questions are whether the approximate methods are convergent? whether they give useful decision rules in the end? My research efforts have been towards throwing light into these questions.
\begin{itemize}
\item The talk is organized as follows
\item First we look at the framework for sequential decision making namely Markov Decision Process
\item We will look at exact dynamic programming which are the blue print algorithms to compute optimal decisions
\item Then we look at the  Approximate dynamic programming discuss the need, goals, issues or gaps with the state of the art and the important questions -some of which I was able address during the course of my research
Then we discuss in detail two of my works
1) based on tropical linear algebra
2) some recent results that further our understanding of approximate linear programming formulation
\end{itemize}

\section{MDP}
We now look at the MDP framework. First we will begin with a motivating example
\subsection{Mouse}
The mouse wants to reach the cheese. It can sense its position in terms of the (x,y) position. We also assume that the environment has some noise in the form of random displacements.
So, what do we mean by decisions here? it is a map $u^*$ from position to the immediate movement of the mouse. Due to the randomness we might need this map for all the possible states. Further, there is a ``temporal" nature to these decisions, i.e., we obtain a sequence of states and actions.
\subsection{Complex Systems}
The paradigm of decision making is ubiquitos and occurs in various applications. Such as atari games or robosoccer which has captured the attention of Artificial Intelligence/Machine learning community, or queuing networks usually of interest to communications community and inventroy control which is of interest to the operations research community. Conceptually, it is better to imagine the problem to be one where we find the human/algorithm receiving observations of the environment and needs to make decisions, so there is this system in loop flavour to these problems.
\subsection{What and What not?}
The system in loop is significantly different from looking at a mere data stream. In these class of problems there are no labels such as right or wrong. One needs to interact with the environment. Thus it is a problem of control of a stochastic dynamical systems where the current decisions affect the future. And the aim is to control the system.
\subsection{MDPs}
The sequential decision making problems are stated in the formal language of Markov Decision Processes. An MDP is given by a five tuple.
The state space $\S$, the action space $\A$, the Markovian transition structure that says given the present the future is independent of the past. We also assume that this behaviour is time homogeneous. Then the reward structure which is a map from the state-action space to reals.\\
Associated with MDP is the notions of policy and value function. A policy is a map that specifies the action selection mechanism and the value function is specifies the expected total discounted reward that is collected by following a given policy. Here, the discount factor ensures that the immediate future is more important than the distant future. And our aim is to achieve optimality, i.e., find the best policy and the best possible value that can be obtained. We call $J_u$ as value function or the value vector and naturally identify it with $\R^{|\S|}$.

\subsection{Paradigm Shift}

Let us now compare the MDP framework with the notion of decision making that we know from machine learning or detection/hypothesis testing.
The classification problem has data in the form of $(x,y)=(feature,class)$ and the aim is learn the rule that maps features to any of the $K$ classes.
Suppose we are given the model know the underlying class conditional distributions, the problem is straightforward. Now, in the case of MDPs this is not the case, i.e., even when we are given the model we still need to compute $u^*$ and this is non-trivial.

\subsection{Curse of Dimensionality}
We know that the complexity is polynomial in the number of states and actions. However, the growth of the state space is the issue. Consider the example of a queue with buffer size $n$, and then a queuing system with $q$ different queues.
\subsection{Curse of Dimensionality}
Due to the curse, it is hard to compute the exact decisions. A way to tackle this predicament is by compressing the problem somehow and then compute and approximate decision. Then the question that we need to answer is how good is the approximate decision rule in comparison to the exact and optimal decision rule
\subsection{Lack of Model}
Further, in many cases the explicit model is not available but only samples via direct interaction. Thus we need to design algorithms that keep updating in an online manner.
And it is important to understand the stability of the iterates of the algorithm.


\section{Exact Dynamic Programming}
\subsection{Bellman Equation}
We said we need to compute the optimal decision and this can be done by computing the optimal value. And the optimal value can be computed by the greedy principle which says the best is current best+ future best. Formally this can be stated as the Bellman equation.
\subsection{Bellman Operator}
\subsection{DP algorithms}
We now look at the basic algorithms which form the blue print to compute optimal decisions. Value iteration is based on the power method. Policy iteration is based on the idea of policy improvement. We will look at the linear programming formulation in a much more detailed fashion.
The key take away is the all these methods are convergent and exact.

\section{Approximate Dynamic Programming}
\subsection{Definitions}
We will now look at certain norms that will be useful.
Here the modified $L_{\infty,\psi}$ is going to be useful for us. It allows to control the error in different parts of the state space differently.
We say that a function $f$ is superharmonic to $g$ when $f\geq g$
\subsection{Defnitions}
The notion of maximal inflation is to compute the one step growth of the function. This is usually defined in the context of stability, but here we will use it for approximablility.
A Lyapunov function is one that get contracted by the system, so to speak. And finally contraction property in the $L_\infty$ norm.
\subsection{ADP goals}
We will now formalize the notions of approximate decision making. Formally this paradigm is called approximate dynamic programming. So why do we need it. As we mentioned before and even in the small toy grid example that we looked at, we need to populate the table. This might be tedious when there are a large number of states. So, we are not going to do that in practice. Instead, we are going to compress and somehow want to get away with less computations. So, ADP is just the name for dynamic programming + compression. Here, we first compute an approximate value function $\tilde{J}$ and then compute say a greedy policy $\tilde{U}$. And we know from an earlier result (the reference here is a book) that the if the prediction error is controlled then the loss of performance can also be controlled.

There are also other methods that directly handle the policy. While some of my other work related to stability of RL algorithms is related to this policy compression business, I will not be discussing it in this talk.
\subsection{How to compress?}
We look in the direction of linear regression where the idea is to express the target as a linear combination of a set of pre-selected basis functions. Here, $\Phi$ is a feature matrix and $\tilde{r}$ is a weight vector to be learnt. And, the idea is chose fewer basis functions than the size of the state space. Very closely related to regression is the projection operator $\Pi$ which can be expressed as this optimization problem.
\subsection{How to compute $\tj$?}
Now that we have compressed the value function, it remains for us to still compute it. So we look at the exact dynamic programming algorithms and then the see whether we can plug in this approximation. First, we can check whether an approximate value iteration method will work. We hit our first road block due to the fact that there is norm incompatibility between $T$ and $\Pi$. However, restricted conditions were given by Melo et al, where it says that the features should have some convexity property. So, we move onto check the status of approximate policy iteration, and yet again we hit a road block, where the bound on the prediction error is only in the two norm. Though all norms are equivalent in finite dimensions, controlling the $L_\infty$ norm is more tighter. So policy improvement is not guaranteed and hence the procedure can oscillate amongst bad policies.
\subsection{Approximate Linear Programming}
The approximation idea can also be combined with linear programming. The Approximate Linear Program was first introduced by \cite{schweitzer1985generalized} and later the performance bounds were shown by \cite{de2003linear}. While we will see this formulation in a bit more detail, what we would like to note here is that, even in this formulation we hit a road block in the form of a large number of constraints.
\section{Research Goals}
With this perspective of dynamic programming and then approximate dynamic programming, we know that we have to compress one way or the other and compute an approximate value function. However, by now we are also aware of the road blocks and the goal is to remove them as much as possible or throw more light on where the structure is breaking.

\subsection{Research Contributions}
We now look at some of the highlights contributions that I could make to these questions. One of the first stepping stones was to recognize that tropical linearity has been looked in the context of ADP with deterministic systems. Here, one deals with different projection onto sub-semimodules. The crux is that when one works with tropical linearity as we will see contraction maps and fixed points occur much more naturally. The second most important leap is to combine cleverly the ideas in tropical linear algebra and conventional algebra to study the approximate linear program. And further, derive geometric conditions that will guide us in addressing the problem of constraint reduction. In all of this superharmonicity plays a key role.

\section{ADP in Tropical Algebra}

\subsection{Tropical Intro}
We now look at the variants of tropical algebra namely $\minp$ and $\maxp$ semirings.

\subsection{Tropical literature}
Bacelli et al wrote this book on Synchronization and linearity which deals with synchronization systems. We will briefly look at what they are. Then Cohen etal describes how to project onto sub-semimodules (which are like subspaces but with respect to a semi-ring). Akian et al used the projection ideas for deterministic control, follwed by McEneany et al who applied it for HJB PDEs and then Gaubert et al studied some pruning methods with such basis.

\section{$\minp$ linear basis}

We now define the linear basis in $\minp$. This linear span is formally known as a sub-semimodule. The  The projection operator finds the least superharmonic function from the span. One can also do weak projections to ease the computational overhead.
\subsection{Choice of $\minp$}
Now these are the properties,
It is slightly strange that the deterministic cost problem is $\minp$, however we are using it for reward setting. This has largely to do with the fact that $T$ can be unfurled into a bunch of superharmonic inequalities. This make $\Pi^W_M T$ a contraction map.
\subsection{Results}
We now look at the algorithm and the performance guarantees. The saleint features are bound in $L_\infty$ norm (remember the relation between prediction error and performance loss), guranteed convergence (no policy oscillation etc), curse of dimensionality is truly reduced, and the error has two parts, the first one is the approximation error due to the fact that we are restricted to the linear span and then second term is due to weak projection.
\subsection{Experiments}
We ran the experiments on mountain car domain which is a benchmark problem, where there is an underpowered car and it has to accelerate its way out of the valley. The optimum strategy would be to accelerate in the direction of the velocity (not necessarily the position) and swing up. We made use of quadratic functions and approximate the value function.
\subsection{Research Goals}
We said we wanted to understand how to compress in the right fashion and we were able to push the understanding a bit further.

\section{LP}
The LP formulation has  $|\S|$ variables $|\S||\A|$ constraints. Here, $c\geq 0 \in \R^{|\S|}$ is a non-negative vector. Searches in the bag of superharmonic functions $J\geq TJ\geq T^2J\geq J^*$. Complexity is $poly(|\S|,|\A|)$

\subsection{Approximate Linear Programming (ALP)}
As a deviation from the projection idea \cite{schweitzer1985generalized} introduced linear function approximation in the LP formulation. ALP has $k$ variables $|\S||\A|$ constraints and searches in space of {superharmonic} functions {{restricted}} to span of $\Phi$. Later \cite{de2003linear}  settled questions of feasibility and performance. ALP is feasible if $\one \in span(\Phi)$.
$\underbrace{\norm{J^*-\tj}_{1,c}}_{\text{Error in ALP}}\leq \frac{2}{1-\alpha}\underset{r\in \R^k}{\min}\underbrace{||J^*-\Phi r||_{\infty}}_{\text{Best in the basis}}$

\subsection{Saliency of ALP}
\begin{itemize}
\item $\norm{J^*-\tj}_{1,c}$, can choose $c$ to control the error
\item $\underbrace{\norm{J^*-\tj}_{1,c}}_{\text{Error in ALP}}\leq \frac{2}{1-\beta_\psi}\underset{r\in \R^k}{\min}\underbrace{||J^*-\Phi r||_{\infty,\psi}}_{\text{Best in the basis}}$
\item \cite{de2003linear,de2004constraint} choose $\psi$ to be large in un-important parts of state space
\end{itemize}

\subsection{Proofs}
The idea is to start from $\Phi r^*$ the best possible approximation in the basis and then translate in the direction of the vector of all $1$s.
\subsection{Reduced Linear Program}
Was studied by \cite{de2004constraint},
\begin{itemize}
\item $\N$ is chosen to ensure boundedness
\item $\I$ is an index set of cardinality $m$ containing state-action pairs
\item Not superharmonic (can be handled)
\end{itemize}
We can see that the constraint region is strictly bigger due to the relaxation of the constraints.

\subsection{RLP: Details}
$\N$, $\I$ need to be specified properly. $\I$ needs to be sampled from a distribution $\mu^*$, which in turn depends on the optimal policy $u^*$. It just says choose the distribution which is the discounted sum of the visits with respect to the optimal policy, scale it with an appropriate Lyapunov function and use it sample the constraints, then we are good. Further, there are some additional requirements such as $\N$ contains $\tr$ (solution to ALP). The result holds due a combinatorial argument based on the VC-dimensions which requires $m$ to be order of $k$.
\subsection{Constraint Reduction- Gap in Theory}
There are multiple issues with the RLP
\begin{itemize}
\item No apriori way to choose $\N$
\item For unbounded $\N$ the result is vacuous
\item Sampling distribution depends on $u^*$
\item Use of other sampling distribution will have importance sampling ratio, thus the number of samples required will blow up
\item \cite{farias2006tetris,de2003linear,de2004constraint} Empirical results good for variety of sampling distribution (no knowledge of $u^*$) on various domains (Tetris, Queues etc)
\end{itemize}

Thus the main gap in theory is that it does not explain empirical evidence

\subsection{Linearly Relaxed Approximate Linear Program}
Here
\begin{itemize}
\item $W$ is a $m\times |\S||\A|$ matrix with non-negative entries
\item LRALP is has $k$ variables $m$ constraints
\item $W$ can encode sampling as well as linear projection of constraints
\end{itemize}
We removed $\N$ because it is not possible to choose it in an apriori fashion. Instead, we will focus on geometric conditions that ensure boundedness.

\subsection{Novel Contraction Operators}
Using the superharmonic property of the ALP much similar to the one in tropical $\minp$ algebra, we define novel projection operators

\subsection{Main Result - Prediction Error}
Then Main result can be stated as follows
\begin{align*}
\norm{J^*-\hj}_{1,c}\leq \frac{2}{1-\alpha}( 3\norm{J^*-\Phi r^*}_{\infty}+ \norm{\Gamma J^*-\hg J^*}_{\infty})
\end{align*}
\begin{itemize}
\item Error has two parts $i)$ Approximation Error $ii)$ Projection Error
\item The bound is deterministic
\item Factor $3$ is a proof artifact
\item Does not depend on sampling
\item Can be stated in $\norm{\cdot}_{\infty,\psi}$
\end{itemize}


\subsection{Main Result - Constraint Sampling}
We need to bound $\norm{\Gamma J^*-\hg J^*}_{\infty}$ analytically. This can be done using the conic conditions

Let $\S_0\subset \S$ such that the rows of $\Phi$ lies in the conic span of $\Phi(\S_0,\cdot)$ with co-efficient matrix $\Lambda$. Then
\begin{align*}
\norm{ \Gamma J^*-\hg J^* }_{\infty,\psi}\leq (3+\norm{\Lambda \psi}_{\infty,\psi})\inf_r\norm{J^*-\Phi r}_{
\infty,\psi}
\end{align*}

By doing so we achieve the following
\begin{itemize}
\item Geometric condition for linear programs
\item Can ensure for variety of linear approximation such as tile coding, state aggregators, separable bases
\item $|\S_0|$ is purely geometric requirement, unlike $m$ samples for RLP
\end{itemize}


\subsection{Properties of new operators}
The proof was possible because the two operators we defined enjoy the following properties
\begin{itemize}
\item Define $\F_J\eqdef\{\Phi r | \Phi r\geq TJ \}$
\item $\F_J$ contains all functions superharmonic with $TJ$
\item {Monotonicity:} $\Gamma J_1\geq \Gamma J_2, \mb \forall$ $J_1\geq J_2$
\item {Shift:} $\Gamma (J_1+k\one)= \Gamma J_1+ k\one$
\item $\Gamma$ and $\hg$ are $L_{\infty,\psi}$ contraction maps
\end{itemize}


\subsection{Experimental Results}
We ran some experiments to check the validity of the theroy, i.e., whether bounds do show up. For this we chose a queue with $10^4$ states.
We tested it for random matrix, a sampling matrix, an aggregation matrix where nearby states are lumped together, and the results are a shown in the table. We see that but for the random choice, other reasonable choices are doing well indeed.





\end{document}




\end{document}




